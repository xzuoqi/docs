:::tip
This document demonstrates how to run YOLOv5 object detection model inference on Allwinner T527/A733 series chips.
:::

This example uses a pre-trained ONNX format model from [ultralytics/yolov5](https://github.com/ultralytics/yolov5/releases/tag/v6.0) to demonstrate the complete process from model conversion to on-device inference.

Deploying YOLOv5 on the development board involves two main steps

- On the PC side, use [ACUITY Toolkit](./cubie_acuity_sdk#acuity-toolkit) to convert models from different frameworks to NBG format
- On the device side, use awnn API for model inference

## Download ai-sdk Example Repository

<NewCodeBlock tip="X86 PC / Device" type="PC">

```bash
git clone https://github.com/ZIFENG278/ai-sdk.git
```

</NewCodeBlock>

## PC-side Model Conversion

:::tip
Radxa provides a pre-converted `yolov5.nb` model. Users can directly refer to [**On-device YOLOv5 Inference**](#board-side-yolov5-inference) to skip the PC-side model conversion section.
:::

:::tip
The files used in the YOLOv5 example are already included in the `models/yolov5s-sim` directory of the [ai-sdk example repository](https://github.com/ZIFENG278/ai-sdk.git)
:::

- Enter the ACUITY Toolkit Docker container

  For ACUITY Toolkit Docker environment setup, please refer to [ACUITY Toolkit Environment Configuration](./cubie_acuity_env)

  Configure environment variables

      <Tabs>

  <TabItem value="A733">

          <NewCodeBlock tip="X86 Linux PC" type="PC">
          ```bash
          cd ai-sdk/models
          source env.sh v3 # NPU_VERSION
          cp ../scripts/* .
          ```
          </NewCodeBlock>

          </TabItem>

          <TabItem value="T527">

          <NewCodeBlock tip="X86 Linux PC" type="PC">
          ```bash
          cd ai-sdk/models
          source env.sh v2 # NPU_VERSION
          cp ../scripts/* .
          ```
          </NewCodeBlock>

          </TabItem>

      </Tabs>

  :::tip
  Specify NPU_VERSION: use `v3` for A733 and `v2` for T527. Please refer to the [**NPU Version Comparison Table**](cubie_acuity_usage#npu-version-comparison-table) for reference.
  :::

- Download yolov5s ONNX model

  <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  mkdir yolov5s-sim && cd yolov5s-sim
  wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.onnx
  ```

  </NewCodeBlock>

- Fix input dimensions

  NPU inference only accepts fixed input dimensions. Here we use onnxsim to fix the input dimensions

  <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  pip3 install onnxsim onnxruntime
  onnxsim yolov5s.onnx yolov5s-sim.onnx --overwrite-input-shape 1,3,640,640
  ```

  </NewCodeBlock>

- Create quantization calibration dataset

  Use an appropriate number of images to create a quantization calibration dataset. The image paths should be saved in `dataset.txt`

  <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  vim dataset.txt
  ```

  </NewCodeBlock>

  ```vim
  images/COCO_train2014_000000000529.jpg
  images/COCO_train2014_000000001183.jpg
  images/COCO_train2014_000000002349.jpg
  images/COCO_train2014_000000003685.jpg
  images/COCO_train2014_000000004463.jpg
  images/dog.jpg
  ```

- Create model input/output files

  You can use [netron](https://netron.app/) to confirm the input/output names of the ONNX model

   <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  vim inputs_outputs.txt
  ```

   </NewCodeBlock>

  ```vim
  --inputs images --input-size-list '3,640,640' --outputs '350 498 646'
  ```

  {" "}

  <div style={{ textAlign: "center" }}>
    <img src="/img/cubie/a7a/yolov5_input.webp" />
    yolov5s in/output name
  </div>

- Directory contains files

  ```bash
  .
  |-- dataset.txt
  |-- images
  |   |-- COCO_train2014_000000000529.jpg
  |   |-- COCO_train2014_000000001183.jpg
  |   |-- COCO_train2014_000000002349.jpg
  |   |-- COCO_train2014_000000003685.jpg
  |   |-- COCO_train2014_000000004463.jpg
  |   `-- dog.jpg
  |-- inputs_outputs.txt
  |-- yolov5s-sim.onnx
  ```

- Parse the model

  :::tip
  The pegasus script is located in ai-sdk/scripts and can be copied to the models directory
  :::

  Use `pegasus_import.sh` to parse the model into IR expressions, which will generate `yolov5s-sim.json` containing the model structure and `yolov5s-sim.data` containing the model weights

   <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  ./pegasus_import.sh yolov5s-sim/
  ```

   </NewCodeBlock>

- Modify yolov5s-sim_inputmeta.yml file

  Here we modify the scale value according to the formula: scale = 1 / std

  ```bash
  scale = 1 / 255
  scale = 0.00392157
  ```

  ```bash
  input_meta:
    databases:
    - path: dataset.txt
      type: TEXT
      ports:
      - lid: images_208
        category: image
        dtype: float32
        sparse: false
        tensor_name:
        layout: nchw
        shape:
        - 1
        - 3
        - 640
        - 640
        fitting: scale
        preprocess:
          reverse_channel: true
          mean:
          - 0
          - 0
          - 0
          scale:
          - 0.00392157
          - 0.00392157
          - 0.00392157
          preproc_node_params:
            add_preproc_node: false
            preproc_type: IMAGE_RGB
            # preproc_dtype_converter:
              # quantizer: asymmetric_affine
              # qtype: uint8
              # scale: 1.0
              # zero_point: 0
            preproc_image_size:
            - 640
            - 640
            preproc_crop:
              enable_preproc_crop: false
              crop_rect:
              - 0
              - 0
              - 640
              - 640
            preproc_perm:
            - 0
            - 1
            - 2
            - 3
        redirect_to_output: false
  ```

- Quantize the model

  Use `pegasus_quantize.sh` to quantize the model into uint8 type

   <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  ./pegasus_quantize.sh yolov5s-sim/ uint8 10
  ```

   </NewCodeBlock>

- Compile the model

  Use `./pegasus_export_ovx.sh` to compile the model into NBG model format

  <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  ./pegasus_export_ovx.sh yolov5s-sim/ uint8
  ```

  </NewCodeBlock>

  The NBG model is saved in `yolov5s-sim/wksp/yolov5s-sim_uint8_nbg_unify/network_binary.nb`

## Board-side YOLOv5 Inference

Navigate to the yolov5 example code file directory path

<NewCodeBlock tip="Device" type="device">

```bash
cd ai-sdk/examples/yolov5
```

</NewCodeBlock>

### Compile the Example

<Tabs>
    <TabItem value="A733">

    <NewCodeBlock tip="Device" type="device">

    ```bash
    make AI_SDK_PLATFORM=a733
    make install AI_SDK_PLATFORM=a733 INSTALL_PREFIX=./
    ```

    </NewCodeBlock>

    </TabItem>

    <TabItem value="T527">

    <NewCodeBlock tip="Device" type="device">
    ```bash
    make AI_SDK_PLATFORM=t527
    make install AI_SDK_PLATFORM=t527 INSTALL_PREFIX=./
    ```
    </NewCodeBlock>

    </TabItem>

</Tabs>

Parameter description:

`AI_SDK_PLATFORM`: Specify SoC, optional **`a733`**, **`t527`**

`INSTALL_PREFIX`: Specify installation path

### Run Example

Import environment variables

<Tabs>
    <TabItem value="A733">

    <NewCodeBlock tip="Device" type="device">

    ```bash
    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/rock/ai-sdk/viplite-tina/lib/aarch64-none-linux-gnu/v2.0 # NPU_SW_VERSION
    ```

    </NewCodeBlock>

    </TabItem>

    <TabItem value="T527">

    <NewCodeBlock tip="Device" type="device">
    ```bash
    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/rock/ai-sdk/viplite-tina/lib/aarch64-none-linux-gnu/v1.13 # NPU_SW_VERSION
    ```
    </NewCodeBlock>

    </TabItem>

</Tabs>

:::tip
Specify NPU_SW_VERSION: use `v2.0` for A733 and `v1.13` for T527. Please refer to the [**NPU Version Comparison Table**](./cubie_acuity_usage#npu-version-comparison-table) for NPU information.
:::

Navigate to the example installation directory

<NewCodeBlock tip="Device" type="device">

```bash
cd $INSTALL_PREFIX/etc/npu/yolov5
# ./yolov5 nbg_model input_picture
./yolov5 ./model/yolov5.nb ./input_data/dog.jpg
```

</NewCodeBlock>

:::tip
The example will automatically install the yolov5.nb model provided by Radxa. You can manually specify the path to your own converted NBG model here.
:::

```bash
(.venv) rock@radxa-cubie-a7a:~/ai-sdk/examples/yolov5/etc/npu/yolov5$ ./yolov5 ./model/network_binary.nb ./input_data/dog.jpg
./yolov5 nbg input
VIPLite driver software version 2.0.3.2-AW-2024-08-30
viplite init OK.
VIPLite driver version=0x00020003...
VIP cid=0x1000003b, device_count=1
* device[0] core_count=1
awnn_init total: 5.49 ms.
  vip_create_network ./model/network_binary.nb: 3.96 ms.
input 0 dim 640 640 3 1, data_format=2, name=input/output[0], elements=1833508979, scale=0.003922, zero_point=0
create input buffer 0: 1228800
output 0 dim 85 80 80 3 1, data_format=2, name=uid_5_out_0, elements=1632000, scale=0.085919, zero_point=211
create output buffer 0: 1632000
output 1 dim 85 40 40 3 1, data_format=2, name=uid_4_out_0, elements=408000, scale=0.071616, zero_point=204
create output buffer 1: 408000
output 2 dim 85 20 20 3 1, data_format=2, name=uid_3_out_0, elements=102000, scale=0.072006, zero_point=196
create output buffer 2: 102000
memory pool size=3892224 bytes
  load_param ./model/network_binary.nb: 0.97 ms.
  prepare network ./model/network_binary.nb: 2.56 ms.
  set network io ./model/network_binary.nb: 0.01 ms.
awnn_create total: 7.55 ms.
yolov5_preprocess.cpp run.
memcpy(0xffff89621000, 0xffff886f8010, 1228800)  load_input_data: 0.33 ms.
  vip_flush_buffer input: 0.02 ms.
awnn_set_input_buffers total: 0.38 ms.
  vip_run_network: 17.07 ms.
  vip_flush_buffer output: 0.01 ms.
    int8/uint8 1632000 memcpy: 2.72 ms.
    int8/uint8 408000 memcpy: 0.46 ms.
    int8/uint8 102000 memcpy: 0.11 ms.
  tensor to fp: 28.64 ms.
awnn_run total: 45.75 ms.
yolov5_postprocess.cpp run.
detection num: 3
16:  86%, [ 130,  222,  312,  546], dog
 7:  59%, [ 469,   78,  692,  171], truck
 1:  53%, [ 158,  133,  560,  424], bicycle
awnn_destroy total: 1.95 ms.
awnn_uninit total: 0.66 ms.
```

The inference results are saved in result.png

<div style={{ textAlign: "center" }}>
  <img src="/img/cubie/a7a/yolov5_result.webp" />
  yolov5s demo output
</div>
